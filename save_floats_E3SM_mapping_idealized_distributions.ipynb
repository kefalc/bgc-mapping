{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816a8784-8c24-4bb5-b790-052766d66fa8",
   "metadata": {},
   "source": [
    "# save synthetic float data for mapping as netcdf\n",
    "#\n",
    "# Note: Here, instead of requiring that a float stays within a region for the whole chosen time period, use all profiles in the region in the chosen time window\n",
    "#\n",
    "# USER SETTINGS: \n",
    "# see second cell in script\n",
    "# the following can be set by user: years, depth levels, temporal frequency, region, variables\n",
    "# (variables can be defined by user but number (3) is currently hard-coded (default: temperature, salinity & oxygen)\n",
    "# there is also the option to randomly subselect a certain percentage of the data\n",
    "# all variables are interpolated to the chosen depth levels\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860ad28a-7d61-4bd1-ab78-71fc1ca51b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/global/homes/c/cnissen/scripts/seawater-3.3.4/seawater/')\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "#from seawater import dist\n",
    "#import seawater as sw\n",
    "import matplotlib.path as mpath\n",
    "from cartopy.util import add_cyclic_point\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.ticker import (LongitudeFormatter, LatitudeFormatter,\n",
    "                                LatitudeLocator)\n",
    "from numba import njit\n",
    "import time\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "from tqdm import tqdm\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from scipy import interpolate\n",
    "from netCDF4 import Dataset, MFDataset\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d4fbac-e76e-41cb-b60f-1b09e94de541",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year1,year2: 2010 2019\n",
      "depth_interp [   -5   -10  -100  -250  -500 -1000 -1500 -3000]\n"
     ]
    }
   ],
   "source": [
    "#-----\n",
    "# some user-defined settings\n",
    "#-----\n",
    "# NOTE: the location of the E3SM float data is currently hard-coded further down in the script\n",
    "\n",
    "# where to store the resulting *nc file?\n",
    "savepath     = '/global/homes/k/kefalc/code/data_files/Final/'\n",
    "# check existence of paths\n",
    "if not os.path.exists(savepath):\n",
    "    print ('Created '+savepath)\n",
    "    os.makedirs(savepath)\n",
    "\n",
    "# where to store the final plot showing the distirbution of the data?\n",
    "savepath_plots     = '/global/homes/k/kefalc/code/plots/'\n",
    "# check existence of paths\n",
    "if not os.path.exists(savepath_plots):\n",
    "    print ('Created '+savepath_plots)\n",
    "    os.makedirs(savepath_plots)\n",
    "    \n",
    "# optional: instead of talking all available profiles for a given region and time period, \n",
    "# the user can here select to only store xx% of all available profiles\n",
    "# NOTE: the temporal distribution is kept unchanged, i.e., xx% of all profiles are chosen for each month and each year\n",
    "reduce_floats = True\n",
    "keep_how_many = 30 # in percent\n",
    "\n",
    "# choose how often a float should sample (--> one sample every xx days)\n",
    "# NOTE: the script further down assumes that there are 6 samples per day in E3SM output\n",
    "xx_daily = 10 # 10-daily sampling\n",
    "\n",
    "# define name of the region\n",
    "# NOTE: this string needs to be defined further down in script to restrict the profiles correctly\n",
    "# currently, I have defined different versions of the SO\n",
    "region_string = 'SO_30S' # used in filenames and to restrict data to a specific region\n",
    "\n",
    "# choose years\n",
    "year1,year2 = 2010,2019  # currently, we have E3SM data from 1980-2019\n",
    "print('year1,year2:',year1,year2)\n",
    "\n",
    "# choose a list of target depth levels, interpolate (depth levels must be negative as in E3SM files)\n",
    "# NOTE: extrapolation is currently set to \"False\" in script below\n",
    "#   i.e., that will not be any data when it is shallower than maximum depth chosen here\n",
    "#   further, the shallowest level might also be NaN when it is shallower than the shallowest level available for E3SM float\n",
    "depth_interp=np.array([-5,-10,-100,-250,-500,-1000,-1500,-3000])\n",
    "print('depth_interp',depth_interp)\n",
    "    \n",
    "# variables to process\n",
    "# NOTE: while three variables are currently hard-coded in the script, the variables themselves are interchangeable\n",
    "vari_temp = 'particleColumnTemperature'\n",
    "vari_salt = 'particleColumnSalinity'\n",
    "vari_oxy  = 'particleColumnO2'\n",
    "# settings for writing the chosen variables into the final nc file\n",
    "vari1_nc = 'temperature'\n",
    "vari2_nc = 'salinity'\n",
    "vari3_nc = 'oxygen'\n",
    "unit1    = 'degC'\n",
    "unit2    = 'ppt'\n",
    "unit3    = 'mmol m-3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804508aa-4156-43fb-8aa0-0e388182ee11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# some functions... \n",
    "#---\n",
    "\n",
    "# convert day/month to day_of_the_year\n",
    "# functions from here: https://stackoverflow.com/questions/620305/convert-year-month-day-to-day-of-year-in-python\n",
    "\n",
    "def is_leap_year(year):\n",
    "    \"\"\" if year is a leap year return True\n",
    "        else return False \"\"\"\n",
    "    if year % 100 == 0:\n",
    "        return year % 400 == 0\n",
    "    return year % 4 == 0\n",
    "\n",
    "def doy(Y,M,D):\n",
    "    \"\"\" given year, month, day return day of year\n",
    "        Astronomical Algorithms, Jean Meeus, 2d ed, 1998, chap 7 \"\"\"\n",
    "    if is_leap_year(Y):\n",
    "        K = 1\n",
    "    else:\n",
    "        K = 2\n",
    "    N = int((275 * M) / 9.0) - K * int((M + 9) / 12.0) + D - 30\n",
    "    return N\n",
    "\n",
    "def ymd(Y,N):\n",
    "    \"\"\" given year = Y and day of year = N, return year, month, day\n",
    "        Astronomical Algorithms, Jean Meeus, 2d ed, 1998, chap 7 \"\"\"    \n",
    "    if is_leap_year(Y):\n",
    "        K = 1\n",
    "    else:\n",
    "        K = 2\n",
    "    M = int((9 * (K + N)) / 275.0 + 0.98)\n",
    "    if N < 32:\n",
    "        M = 1\n",
    "    D = N - int((275 * M) / 9.0) + K * int((M + 9) / 12.0) + 30\n",
    "    return Y, M, D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921e3ffc-bf9f-46ba-ba3d-715b70fb3924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010 2011 2012 2013 2014 2015 2016 2017 2018 2019]\n",
      "(21900,) (21900,) (21900,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:22,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (2190, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:04<00:19,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (4380, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:07<00:17,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (6570, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:10<00:15,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (8760, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:12<00:13,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (10950, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:15<00:10,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (13140, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:18<00:08,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (15330, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:22<00:06,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (17520, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:26<00:03,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (19710, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_all.shape (21900, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#----\n",
    "# load E3SM float data (positions only for now)\n",
    "#----\n",
    "# lat,lon\n",
    "# construct arrays with year, day_of_the_year, hour\n",
    "\n",
    "path1 = '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/nonProfilingFloats/'\n",
    "year_list = np.arange(year1,year2+1,1) \n",
    "print(year_list)\n",
    "\n",
    "rad_to_deg = 180.0/np.pi\n",
    "\n",
    "hours_e3sm = np.asarray(len(year_list)*365*[0,4,8,12,16,20]) # num_years * 365 * 4-hourly sampling\n",
    "doy_e3sm   = np.sort(np.asarray(6*list(np.arange(1,365+1,1)))) # repeat each day 6 times (6 samples per day)\n",
    "doy_e3sm   = np.asarray(len(year_list)*list(doy_e3sm))\n",
    "years_e3sm = np.sort(np.asarray(365*6*list(year_list)))\n",
    "print(hours_e3sm.shape,doy_e3sm.shape,years_e3sm.shape)\n",
    "\n",
    "for yy in tqdm(range(0,len(year_list))):\n",
    "    \n",
    "    f1   = xr.open_dataset(path1+'nonProfilingFloats.year'+str(year_list[yy])+'all.nc')\n",
    "    lat1 = f1['latParticle'].values*rad_to_deg\n",
    "    lon1 = f1['lonParticle'].values*rad_to_deg\n",
    "    floatid1 = f1['indexToParticleID'].astype(int).values\n",
    "    \n",
    "    f1.close()\n",
    "    \n",
    "    if yy==0:\n",
    "        lat_all = lat1\n",
    "        lon_all = lon1\n",
    "        floatid_all = floatid1\n",
    "    else:\n",
    "        lat_all = np.concatenate((lat_all,lat1))\n",
    "        lon_all = np.concatenate((lon_all,lon1))\n",
    "        floatid_all = np.concatenate((floatid_all, floatid1))\n",
    "    del lat1,lon1, floatid1\n",
    "    print('lat_all.shape',lat_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb171901-8ed1-4885-936f-89ee437552f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 ...\n",
      "2010 ...\n",
      "2013 ...\n",
      "2015 ...\n",
      "2019 ...\n",
      "2017 ...\n",
      "2018 ...\n",
      "2012 ...\n",
      "2014 ...\n",
      "2016 ...\n",
      "['/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2010all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2011all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2012all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2013all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2014all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2015all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2016all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2017all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2018all.nc'\n",
      " '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/profilingFloats.year2019all.nc']\n",
      "<xarray.DataArray 'particleColumnDepth' (record: 21900, nVertLevels: 60,\n",
      "                                         nParticles: 3544)>\n",
      "dask.array<concatenate, shape=(21900, 60, 3544), dtype=float64, chunksize=(2190, 60, 3544), chunktype=numpy.ndarray>\n",
      "Dimensions without coordinates: record, nVertLevels, nParticles\n",
      "Attributes:\n",
      "    long_name:     particle column depth\n",
      "    units:         m\n",
      "    cell_methods:  Time: mean\n",
      "<xarray.DataArray 'particleColumnTemperature' (record: 21900, nVertLevels: 60,\n",
      "                                               nParticles: 3544)>\n",
      "dask.array<concatenate, shape=(21900, 60, 3544), dtype=float32, chunksize=(2190, 60, 3544), chunktype=numpy.ndarray>\n",
      "Dimensions without coordinates: record, nVertLevels, nParticles\n",
      "Attributes:\n",
      "    long_name:     particle column temperature\n",
      "    units:         degC\n",
      "    cell_methods:  Time: mean\n",
      "<xarray.DataArray 'particleColumnSalinity' (record: 21900, nVertLevels: 60,\n",
      "                                            nParticles: 3544)>\n",
      "dask.array<concatenate, shape=(21900, 60, 3544), dtype=float64, chunksize=(2190, 60, 3544), chunktype=numpy.ndarray>\n",
      "Dimensions without coordinates: record, nVertLevels, nParticles\n",
      "Attributes:\n",
      "    long_name:     particle column salinity\n",
      "    units:         psu\n",
      "    cell_methods:  Time: mean\n",
      "<xarray.DataArray 'particleColumnO2' (record: 21900, nVertLevels: 60,\n",
      "                                      nParticles: 3544)>\n",
      "dask.array<concatenate, shape=(21900, 60, 3544), dtype=float64, chunksize=(2190, 60, 3544), chunktype=numpy.ndarray>\n",
      "Dimensions without coordinates: record, nVertLevels, nParticles\n",
      "Attributes:\n",
      "    long_name:     particle column O2\n",
      "    units:         mmol/m3\n",
      "    cell_methods:  Time: mean\n"
     ]
    }
   ],
   "source": [
    "#----\n",
    "# load E3SM files with tracer information\n",
    "#----\n",
    "# use the indices for all colocated E3SM float data points\n",
    "# use these when reading in tracer field\n",
    "#\n",
    "\n",
    "path1 = '/global/cfs/cdirs/m4003/maltrud/southernOceanRefined/profilingFloats/'\n",
    "year_list = np.arange(year1,year2+1,1) \n",
    "\n",
    "# get list of files for chosen years\n",
    "files_all = []\n",
    "for file in glob.glob(path1+\"/*.nc\"):\n",
    "    files_all.append(file) # get all files\n",
    "# reduce to files for chosen years\n",
    "filelist = []\n",
    "for ff in range(0,len(files_all)):\n",
    "    for yy in range(0,len(year_list)):\n",
    "        if str(year_list[yy]) in files_all[ff]: \n",
    "            print(year_list[yy],'...')\n",
    "            try:\n",
    "                filelist = np.concatenate((filelist,[files_all[ff]]))\n",
    "            except: \n",
    "                filelist = [files_all[ff]]\n",
    "filelist = np.sort(filelist) \n",
    "print(filelist) \n",
    "\n",
    "# load files of chosen years\n",
    "ff = xr.open_mfdataset(filelist,concat_dim='record',combine='nested')\n",
    "#print(ff)\n",
    "\n",
    "# depth levels of floats (varying in space and time because model uses zstar coordinates!)\n",
    "ff_depth = ff['particleColumnDepth']\n",
    "print(ff_depth)\n",
    "\n",
    "# temperature as sampled by floats\n",
    "ff_temp = ff[vari_temp]\n",
    "print(ff_temp)\n",
    "\n",
    "# salinity as sampled by floats\n",
    "ff_salt = ff[vari_salt]\n",
    "print(ff_salt)\n",
    "\n",
    "# oxygen as sampled by floats\n",
    "ff_o2 = ff[vari_oxy]\n",
    "print(ff_o2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9188140-9f5f-4266-87ec-09f797951a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21900, 3544)\n",
      "7.225619220581268e-07 360.0000066217054\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# REDUCE DATA in space: only keep profiles within a certain region\n",
    "#---\n",
    "print(lon_all.shape)\n",
    "\n",
    "if region_string=='SO_30S':\n",
    "    latmin = -90 #-69.966\n",
    "    latmax = -30 #-40.00035\n",
    "    lonmin = 0 #-149.99954\n",
    "    lonmax = 360 #-120.00199890136719\n",
    "elif region_string=='SO_40S':\n",
    "    latmin = -90 #-69.966\n",
    "    latmax = -40 #-40.00035\n",
    "    lonmin = 0 #-149.99954\n",
    "    lonmax = 360 #-120.00199890136719\n",
    "elif region_string=='SO_50S':\n",
    "    latmin = -90 #-69.966\n",
    "    latmax = -50 #-40.00035\n",
    "    lonmin = 0 #-149.99954\n",
    "    lonmax = 360 #-120.00199890136719\n",
    "elif region_string=='SO_60S':\n",
    "    latmin = -90 #-69.966\n",
    "    latmax = -60 #-40.00035\n",
    "    lonmin = 0 #-149.99954\n",
    "    lonmax = 360 #-120.00199890136719\n",
    "elif region_string=='SO_Test':\n",
    "    latmin = -65\n",
    "    latmax = -35\n",
    "    lonmin = 215 ##-145\n",
    "    lonmax = 235 ##-125 ##includes 5 extra degrees of lat/lon to account for border regions\n",
    "\n",
    "# create a mask (data to keep: 1, data to discard: 0)\n",
    "mask_SO = np.ones_like(lon_all)\n",
    "mask_SO[lon_all>lonmax] = 0 #= np.ones_like(lon_all)\n",
    "mask_SO[lon_all<lonmin] = 0 ##needed for test area but not for entire SO\n",
    "mask_SO[lat_all>latmax] = 0 # set locations outside of SO to zero\n",
    "mask_SO[lat_all<latmin] = 0 ##needed for test area but not for entire SO\n",
    "\n",
    "# NOTE: no selection based on lon yet! Choose all longitudes for now.\n",
    "# When selecting based on lon, carefully check min/max of lon array\n",
    "print(np.min(lon_all),np.max(lon_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ec3945-bcaa-4401-8a3a-2506604dff9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_time, num_depths, num_floats: 21900 60 3544\n",
      "min/max ind_start: 0 59\n",
      "(3544,) [53 51 59 ... 31 12 25]\n",
      "(21900, 3544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3544/3544 [00:00<00:00, 285391.44it/s]\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# REDUCE DATA in time: \n",
    "#---\n",
    "# create a mask to select only certain data for each float (speed up the data selection and interpolation in next step)\n",
    "\n",
    "#---\n",
    "# XX-daily, e.g., 10-daily: don't sample all floats on day 1,11,21 etc (instead, sample some at day 5,15,25, others at 2,12,22 etc)\n",
    "#---\n",
    "# account for the fact that we have multiple float observations per day in SO run!\n",
    "\n",
    "nn = xx_daily*6   # output is 4-hourly for SO run (6 data points per day)\n",
    "num_time   = ff_o2.shape[0]\n",
    "num_depths = ff_o2.shape[1]\n",
    "num_floats = ff_o2.shape[2]\n",
    "print('num_time, num_depths, num_floats:',num_time,num_depths,num_floats)\n",
    "\n",
    "# get num_floats random numbers between 0-(10*6-1) (10 daily), 0-(5*6-1) (5-daily)\n",
    "ind_start = np.random.choice(np.arange(0,nn), size=num_floats, replace=True)\n",
    "print('min/max ind_start:',np.min(ind_start),np.max(ind_start))\n",
    "print(ind_start.shape,ind_start)\n",
    "\n",
    "# create mask\n",
    "mask_time = np.zeros_like(mask_SO)\n",
    "print(mask_time.shape)\n",
    "for fl in tqdm(range(0,num_floats)):\n",
    "    mask_time[ind_start[fl]::nn,fl] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd16c31-5968-4dbd-bf41-0e38d2f0bd2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [2:51:41<00:00,  2.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 50min 3s, sys: 24min 39s, total: 2h 14min 43s\n",
      "Wall time: 2h 51min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#---\n",
    "# extract tracer data for chosen subregion from synthetic float obs\n",
    "#---\n",
    "# NOTE: It takes too long to do the interpolation for all data first\n",
    "# Therefore, integrate the data selection step into the vertical interpolation step\n",
    "#  --> only do calculation for locations within the region selected above\n",
    "#  --> only do calculation for times identified above\n",
    "#\n",
    "# vertical interpolation is following example in \"create_BGC_netcdf.ipynb\" (a script Kristen gave me)\n",
    "\n",
    "def pc_interpolation(pressure, parameter):\n",
    "    interp_values = interpolate.pchip_interpolate(pressure, temp, pres_interp)\n",
    "    return(interp_values)\n",
    "\n",
    "ind_pp = num_time # I only processed some time steps for testing --> set this to num_time if all data should be processed\n",
    "\n",
    "# loop over all float data points but only compute if both mask_SO and mask_time are 1 \n",
    "temp_interp = np.nan*np.ones([num_time,len(depth_interp),num_floats]) \n",
    "salt_interp = np.nan*np.ones([num_time,len(depth_interp),num_floats]) \n",
    "o2_interp   = np.nan*np.ones([num_time,len(depth_interp),num_floats]) \n",
    "for tt in tqdm(range(0,ind_pp)):\n",
    "    for fl in range(0,num_floats):\n",
    "        if (mask_SO[tt,fl]==1) & (mask_time[tt,fl]==1):\n",
    "            #print(fl)\n",
    "            temp_aux  = ff_temp.isel(record=tt).isel(nParticles=fl).values.flatten() #adjust this for different variables\n",
    "            salt_aux  = ff_salt.isel(record=tt).isel(nParticles=fl).values.flatten() #adjust this for different variables\n",
    "            o2_aux    = ff_o2.isel(record=tt).isel(nParticles=fl).values.flatten() #adjust this for different variables\n",
    "            depth_aux = ff_depth.isel(record=tt).isel(nParticles=fl).values.flatten() #adjust this for different variables\n",
    "          #  print(temp_aux)\n",
    "          #  print(depth_aux)\n",
    "            # remove all missing data (honestly, I don't fully understand why some data points are -2...)\n",
    "            temp_aux  = temp_aux[o2_aux!=-1] # remove all missing data (set to -1 in E3SM data)\n",
    "            salt_aux  = salt_aux[o2_aux!=-1] # remove all missing data (set to -1 in E3SM data)\n",
    "            o2_aux    = o2_aux[o2_aux!=-1] # remove all missing data (set to -1 in E3SM data)\n",
    "            depth_aux = depth_aux[depth_aux!=-1]\n",
    "            temp_aux  = temp_aux[o2_aux!=-2] # remove all missing data (set to -1 in E3SM data)\n",
    "            salt_aux  = salt_aux[o2_aux!=-2] # remove all missing data (set to -1 in E3SM data)\n",
    "            o2_aux    = o2_aux[o2_aux!=-2] # remove all missing data (set to -1 in E3SM data)\n",
    "            depth_aux = depth_aux[depth_aux!=-2]\n",
    "            temp_aux  = temp_aux[o2_aux!=0] # remove all missing data (might be 0 after above steps)\n",
    "            salt_aux  = salt_aux[o2_aux!=0] # remove all missing data (might be 0 after above steps)\n",
    "            o2_aux    = o2_aux[o2_aux!=0] # remove all missing data (might be 0 after above steps)\n",
    "            depth_aux = depth_aux[depth_aux!=0]\n",
    "\n",
    "          #  print(temp_aux)\n",
    "          #  print(depth_aux)\n",
    "            \n",
    "            if len(o2_aux)>0: # only continue if there is any data to be processed\n",
    "                # NOTE: I don't yet fully understand why this is necessary. \n",
    "                # Shouldn't E3SM floats have data everywhere and at all time?\n",
    "                \n",
    "                # temp\n",
    "                pressure_o, biogeochem = zip(*sorted(zip(depth_aux, temp_aux))) # depth levels must be ascending\n",
    "                pressure_o = np.asarray(pressure_o)\n",
    "                biogeochem = np.asarray(biogeochem) \n",
    "                temp_interpolant    = interpolate.PchipInterpolator(pressure_o, biogeochem, extrapolate = False)\n",
    "                temp_interp_values  = temp_interpolant(depth_interp)\n",
    "                temp_interp[tt,:,fl]   = temp_interp_values\n",
    "\n",
    "                # salt\n",
    "                pressure_o, biogeochem = zip(*sorted(zip(depth_aux, salt_aux))) # depth levels must be ascending\n",
    "                pressure_o = np.asarray(pressure_o)\n",
    "                biogeochem = np.asarray(biogeochem) \n",
    "                salt_interpolant    = interpolate.PchipInterpolator(pressure_o, biogeochem, extrapolate = False)\n",
    "                salt_interp_values  = salt_interpolant(depth_interp)\n",
    "                salt_interp[tt,:,fl]   = salt_interp_values\n",
    "\n",
    "                # oxygen\n",
    "                pressure_o, biogeochem = zip(*sorted(zip(depth_aux, o2_aux))) # depth levels must be ascending\n",
    "                pressure_o = np.asarray(pressure_o)\n",
    "                biogeochem = np.asarray(biogeochem) \n",
    "                oxy_interpolant   = interpolate.PchipInterpolator(pressure_o, biogeochem, extrapolate = False)\n",
    "                oxy_interp_values = oxy_interpolant(depth_interp)\n",
    "                o2_interp[tt,:,fl]   = oxy_interp_values\n",
    "                #print('oxy_interp_values',oxy_interp_values)\n",
    "\n",
    "                del pressure_o, biogeochem, oxy_interpolant, oxy_interp_values\n",
    "                del temp_interpolant, temp_interp_values, salt_interpolant, salt_interp_values\n",
    "\n",
    "            del o2_aux, temp_aux, salt_aux, depth_aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10dd427-8b8c-42b9-80e8-22c11c670186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 daily\n",
      "SO_30S\n",
      "temp_interp.shape (21900, 8, 3544)\n",
      "o2_interp.shape (21900, 8, 3544)\n",
      "lat_all.shape (21900, 3544)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77613600, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([    3854,     4064,     4149, ..., 77613107, 77613521, 77613589])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "766306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_to_save1.shape (766306, 8)\n",
      "data_to_save_lat.shape (766306,)\n",
      "data_to_save_lon.shape (766306,)\n",
      "data_to_save2.shape (766306, 8)\n",
      "data_to_save3.shape (766306, 8)\n"
     ]
    }
   ],
   "source": [
    "#----\n",
    "# reorganize data arrays: kick out all locations/time without any data\n",
    "#----\n",
    "print(xx_daily,'daily')\n",
    "print(region_string)\n",
    "\n",
    "print('temp_interp.shape',temp_interp.shape)\n",
    "print('o2_interp.shape',o2_interp.shape)\n",
    "print('lat_all.shape',lat_all.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "aux = np.copy(temp_interp[0:ind_pp,:,:]) # I only processed some time steps for testing \n",
    "aux = np.transpose(aux,[0,2,1])\n",
    "\n",
    "aux = np.reshape(aux,[aux.shape[0]*aux.shape[1],aux.shape[2]]) # num_profile x depth\n",
    "display(aux.shape)\n",
    "ind  = np.nansum(aux,axis=1) # ind is 0 if no data are available\n",
    "ind1 = np.where(ind!=0)[0]\n",
    "display(ind1)\n",
    "display(ind1.size)\n",
    "\n",
    "# TEMPERATURE\n",
    "aux = np.copy(temp_interp[0:ind_pp,:,:]) # I only processed some time steps for testing \n",
    "aux = np.transpose(aux,[0,2,1])\n",
    "aux = np.reshape(aux,[aux.shape[0]*aux.shape[1],aux.shape[2]]) # num_profile x depth\n",
    "# only keep profiles with data\n",
    "ind  = np.nansum(aux,axis=1) # ind is 0 if no data are available\n",
    "ind1 = np.where(ind!=0)[0] # temp can be negative, i.e., ind can be negative! don't search for \">0\" here!\n",
    "data_to_save1 = aux[ind1,:]\n",
    "ind1_noNaN = np.copy(ind1) # keep for creation fo time arrays below\n",
    "print('data_to_save1.shape',data_to_save1.shape)\n",
    "# process lat/lon\n",
    "aux_l = np.copy(lat_all[0:ind_pp,:])\n",
    "aux_l = np.reshape(aux_l,[aux_l.shape[0]*aux_l.shape[1]])\n",
    "data_to_save_lat = aux_l[ind1]\n",
    "print('data_to_save_lat.shape',data_to_save_lat.shape)\n",
    "aux_l = np.copy(lon_all[0:ind_pp,:])\n",
    "aux_l = np.reshape(aux_l,[aux_l.shape[0]*aux_l.shape[1]])\n",
    "data_to_save_lon = aux_l[ind1]\n",
    "idl = np.copy(floatid_all[0:ind_pp,:])\n",
    "idl = np.reshape(idl, [idl.shape[0]*idl.shape[1]])\n",
    "data_to_save_id = idl[ind1]\n",
    "print('data_to_save_lon.shape',data_to_save_lon.shape)\n",
    "del ind,ind1,aux,aux_l\n",
    "\n",
    "# SALINITY\n",
    "aux = np.copy(salt_interp[0:ind_pp,:,:]) # I only processed some time steps for testing \n",
    "aux = np.transpose(aux,[0,2,1])\n",
    "aux = np.reshape(aux,[aux.shape[0]*aux.shape[1],aux.shape[2]]) # num_profile x depth\n",
    "# only keep profiles with data\n",
    "ind  = np.nansum(aux,axis=1) # ind is 0 if no data are available\n",
    "ind1 = np.where(ind!=0)[0]\n",
    "data_to_save2 = aux[ind1,:]\n",
    "print('data_to_save2.shape',data_to_save2.shape)\n",
    "del ind,ind1,aux\n",
    "\n",
    "# OXYGEN\n",
    "aux = np.copy(o2_interp[0:ind_pp,:,:]) # I only processed some time steps for testing \n",
    "aux = np.transpose(aux,[0,2,1])\n",
    "aux = np.reshape(aux,[aux.shape[0]*aux.shape[1],aux.shape[2]]) # num_profile x depth\n",
    "# only keep profiles with data\n",
    "ind  = np.nansum(aux,axis=1) # ind is 0 if no data are available\n",
    "ind1 = np.where(ind!=0)[0]\n",
    "data_to_save3 = aux[ind1,:]\n",
    "print('data_to_save3.shape',data_to_save3.shape)\n",
    "del ind,ind1,aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d19a8b-e84c-4319-a066-ded7425c4db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21900,) [ 1  1  1 ... 31 31 31]\n",
      "(21900,) [ 1.  1.  1. ... 12. 12. 12.]\n",
      "(21900,) [2010. 2010. 2010. ... 2019. 2019. 2019.]\n"
     ]
    }
   ],
   "source": [
    "for yy in range(0,len(year_list)):\n",
    "\n",
    "    # create dates for xx-daily float sampling chosen for the data\n",
    "    months = [31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "    \n",
    "    days0   = np.concatenate((np.arange(1,months[0]+1),np.arange(1,months[1]+1),\\\n",
    "                          np.arange(1,months[2]+1),np.arange(1,months[3]+1),\\\n",
    "                          np.arange(1,months[4]+1),np.arange(1,months[5]+1),\\\n",
    "                          np.arange(1,months[6]+1),np.arange(1,months[7]+1),\\\n",
    "                          np.arange(1,months[8]+1),np.arange(1,months[9]+1),\\\n",
    "                          np.arange(1,months[10]+1),np.arange(1,months[11]+1)))\n",
    "    days0 = np.repeat(days0,6) # 4-hourly data\n",
    "    months0 = np.concatenate((1*np.ones(months[0]),2*np.ones(months[1]),\\\n",
    "                            3*np.ones(months[2]),4*np.ones(months[3]),\\\n",
    "                            5*np.ones(months[4]),6*np.ones(months[5]),\\\n",
    "                            7*np.ones(months[6]),8*np.ones(months[7]),\\\n",
    "                            9*np.ones(months[8]),10*np.ones(months[9]),\\\n",
    "                            11*np.ones(months[10]),12*np.ones(months[11])))\n",
    "    months0 = np.repeat(months0,6) # 4-hourly data\n",
    "    years0 = year_list[yy]*np.ones([365])\n",
    "    years0 = np.repeat(years0,6) # 4-hourly data\n",
    "        \n",
    "    if yy==0: \n",
    "        days_all   = days0\n",
    "        months_all = months0\n",
    "        years_all  = years0\n",
    "    else:\n",
    "        days_all = np.concatenate((days_all,days0))\n",
    "        months_all = np.concatenate((months_all,months0))\n",
    "        years_all = np.concatenate((years_all,years0))\n",
    "    del days0,months0,months,years0\n",
    "\n",
    "print(days_all.shape,days_all)\n",
    "print(months_all.shape,months_all)\n",
    "print(years_all.shape,years_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2791d4f0-f6e4-4bd9-ba4f-d5f36629b219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21900,) [ 1  1  1 ... 31 31 31]\n",
      "(21900,) [ 1.  1.  1. ... 12. 12. 12.]\n",
      "(21900,) [2010. 2010. 2010. ... 2019. 2019. 2019.]\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# CREATE TIME INFO TO STORE IN FILE\n",
    "#---\n",
    "# xx-daily (see above): don't sample all floats on day 1,11,21 etc (instead, sample some on day 5,15,25, others on day 3,13,23 etc)\n",
    "\n",
    "year_list = np.arange(year1,year2+1,1) \n",
    "\n",
    "# creat day/month info for all years\n",
    "for yy in range(0,len(year_list)):\n",
    "\n",
    "    # create dates for xx-daily float sampling chosen for the data\n",
    "    months = [31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "    \n",
    "    days0   = np.concatenate((np.arange(1,months[0]+1),np.arange(1,months[1]+1),\\\n",
    "                          np.arange(1,months[2]+1),np.arange(1,months[3]+1),\\\n",
    "                          np.arange(1,months[4]+1),np.arange(1,months[5]+1),\\\n",
    "                          np.arange(1,months[6]+1),np.arange(1,months[7]+1),\\\n",
    "                          np.arange(1,months[8]+1),np.arange(1,months[9]+1),\\\n",
    "                          np.arange(1,months[10]+1),np.arange(1,months[11]+1)))\n",
    "    days0 = np.repeat(days0,6) # 4-hourly data\n",
    "    months0 = np.concatenate((1*np.ones(months[0]),2*np.ones(months[1]),\\\n",
    "                            3*np.ones(months[2]),4*np.ones(months[3]),\\\n",
    "                            5*np.ones(months[4]),6*np.ones(months[5]),\\\n",
    "                            7*np.ones(months[6]),8*np.ones(months[7]),\\\n",
    "                            9*np.ones(months[8]),10*np.ones(months[9]),\\\n",
    "                            11*np.ones(months[10]),12*np.ones(months[11])))\n",
    "    months0 = np.repeat(months0,6) # 4-hourly data\n",
    "    years0 = year_list[yy]*np.ones([365])\n",
    "    years0 = np.repeat(years0,6) # 4-hourly data\n",
    "        \n",
    "    if yy==0: \n",
    "        days_all   = days0\n",
    "        months_all = months0\n",
    "        years_all  = years0\n",
    "    else:\n",
    "        days_all = np.concatenate((days0,days_all))\n",
    "        months_all = np.concatenate((months0,months_all))\n",
    "        years_all = np.concatenate((years_all, years0))\n",
    "    del days0,months0,months,years0\n",
    "    \n",
    "print(days_all.shape,days_all)\n",
    "print(months_all.shape,months_all)\n",
    "print(years_all.shape,years_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f87e06d-760f-4e07-acbd-a6439ca58319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21900, 3544) (21900, 3544) (21900, 3544)\n",
      "days_all.shape (766306,)\n",
      "months_all.shape (766306,)\n",
      "years_all.shape (766306,)\n",
      "for tt=0: Datetime to nano seconds since epoch: 1.2623472e+18\n",
      "(766306,)\n",
      "min/max time: 1.2623472e+18 1.5777936e+18\n",
      "min/max years: 2010.0 2019.0\n"
     ]
    }
   ],
   "source": [
    "# adapt shape -> include dimension for floats\n",
    "days_all   = np.tile(days_all,[num_floats,1]).transpose() # all floats! (also outside of target region)\n",
    "months_all = np.tile(months_all,[num_floats,1]).transpose()\n",
    "years_all  = np.tile(years_all,[num_floats,1]).transpose()\n",
    "print(days_all.shape,months_all.shape,years_all.shape)\n",
    "\n",
    "# apply time mask to select the correct time entries and reduce arrays to the exisitng profiles\n",
    "days_all   = np.multiply(days_all,mask_time)\n",
    "months_all = np.multiply(months_all,mask_time)\n",
    "years_all  = np.multiply(years_all,mask_time)\n",
    "\n",
    "# reduce TIME arrays\n",
    "# days\n",
    "aux = np.copy(days_all[0:ind_pp,:]) # I only processed some time steps for testing \n",
    "aux = np.reshape(aux,[aux.shape[0]*aux.shape[1]]) # num_profile x depth\n",
    "days_all = aux[ind1_noNaN]\n",
    "print('days_all.shape',days_all.shape)\n",
    "del aux\n",
    "# months\n",
    "aux = np.copy(months_all[0:ind_pp,:]) # I only processed some time steps for testing \n",
    "aux = np.reshape(aux,[aux.shape[0]*aux.shape[1]]) # num_profile x depth\n",
    "months_all = aux[ind1_noNaN]\n",
    "print('months_all.shape',months_all.shape)\n",
    "del aux\n",
    "# years\n",
    "aux = np.copy(years_all[0:ind_pp,:]) # I only processed some time steps for testing \n",
    "aux = np.reshape(aux,[aux.shape[0]*aux.shape[1]]) # num_profile x depth\n",
    "years_all = aux[ind1_noNaN]\n",
    "print('years_all.shape',years_all.shape)\n",
    "del aux\n",
    "\n",
    "# get time in ns\n",
    "time_in_ns = np.nan*np.ones([days_all.shape[0]]) # num_profiles\n",
    "for tt in range(0,days_all.shape[0]):\n",
    "    if ~np.isnan(months_all[tt]):\n",
    "        # input datetime\n",
    "        dt = datetime(int(years_all[tt]), int(months_all[tt]), int(days_all[tt]), 12, 0) # yy-mm-dd-hh-min\n",
    "        # epoch time\n",
    "        epoch_time = datetime(1970, 1, 1)\n",
    "        # subtract Datetime from epoch datetime\n",
    "        time_in_ns[tt] = ((dt - epoch_time).total_seconds())*1e9 # store as \"ns since epoch_time)\n",
    "        if (tt==0):\n",
    "            print('for tt=0: Datetime to nano seconds since epoch:', time_in_ns[tt])\n",
    "print(time_in_ns.shape)\n",
    "#time_in_ns       = np.tile(time_in_ns,[len(ind_zz),1])\n",
    "#time_in_ns       = np.expand_dims(time_in_ns,axis=0)\n",
    "#print(time_in_ns.shape)   \n",
    "print('min/max time:',np.min(time_in_ns),np.max(time_in_ns))\n",
    "print('min/max years:',np.min(years_all),np.max(years_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e53b27d-b3b3-4313-a2c3-b948958cc0de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO_30S\n",
      "Keep all profiles\n",
      "(766306, 8)\n",
      "(766306, 8)\n",
      "(766306, 8)\n",
      "(766306,)\n",
      "(766306,)\n",
      "(766306,)\n",
      "(766306,)\n"
     ]
    }
   ],
   "source": [
    "#----\n",
    "# OPTIONAL: further reduce the data coverage\n",
    "# REDUCE FLOATS BY PROFILE (for optimized distribution)\n",
    "#----\n",
    "# e.g., keep only 50% of all available profiles\n",
    "# NOTE: keep distribution of data across months, i.e., for each year and month, randomly select half of the data points to keep\n",
    "print(region_string)\n",
    "\n",
    "# this is set at the top of the script\n",
    "# however, once all the data is loaded, the user might want to quickly save a variety of distributions\n",
    "# for that, uncomment the two lines below and re-run the code from here until the end for as many times as necessary\n",
    "# (no variables are overwritten, any variables stored in final nc file are defined here)\n",
    "reduce_floats = False\n",
    "keep_how_many = 15 # in percent\n",
    "\n",
    "if reduce_floats:\n",
    "    print('Keep '+str(keep_how_many)+'% of all profiles')\n",
    "\n",
    "    try: \n",
    "        del ind_sel\n",
    "    except: \n",
    "        pass\n",
    "    for yy in year_list:\n",
    "        for mm in range(1,12+1):\n",
    "            # all indices for current year & month\n",
    "            ind_xx = np.where(np.asarray(months_all==mm) & np.asarray(years_all==yy))[0]\n",
    "            # randomly select half\n",
    "            ind_random = np.random.choice(np.arange(0,len(ind_xx)), size=int(ind_xx.shape[0]/(100/keep_how_many)),replace=False)\n",
    "            # collect indices to keep\n",
    "            try: \n",
    "                ind_sel = np.concatenate((ind_sel,ind_xx[ind_random]))\n",
    "            except: # the first time\n",
    "                ind_sel = ind_xx[ind_random]\n",
    "            #print(ind_sel.shape)\n",
    "            del ind_xx,ind_random\n",
    "\n",
    "    print(data_to_save1.shape)\n",
    "    print(data_to_save2.shape)\n",
    "    print(data_to_save3.shape)\n",
    "    print(data_to_save_lat.shape)\n",
    "    print(data_to_save_lon.shape)\n",
    "    print(time_in_ns.shape)\n",
    "    print(data_to_save_id.shape)\n",
    "\n",
    "    data_to_save1b    = data_to_save1[ind_sel,:]\n",
    "    data_to_save2b    = data_to_save2[ind_sel,:]\n",
    "    data_to_save3b    = data_to_save3[ind_sel,:]\n",
    "    data_to_save_latb = data_to_save_lat[ind_sel]\n",
    "    data_to_save_lonb = data_to_save_lon[ind_sel]\n",
    "    time_in_nsb       = time_in_ns[ind_sel]\n",
    "    data_to_save_idb   = data_to_save_id[ind_sel]\n",
    "    \n",
    "else: \n",
    "    print('Keep all profiles')\n",
    "    data_to_save1b    = data_to_save1\n",
    "    data_to_save2b    = data_to_save2\n",
    "    data_to_save3b    = data_to_save3\n",
    "    data_to_save_latb = data_to_save_lat\n",
    "    data_to_save_lonb = data_to_save_lon\n",
    "    time_in_nsb       = time_in_ns\n",
    "    data_to_save_idb  = data_to_save_id\n",
    "    \n",
    "print(data_to_save1b.shape)\n",
    "print(data_to_save2b.shape)\n",
    "print(data_to_save3b.shape)\n",
    "print(data_to_save_latb.shape)\n",
    "print(data_to_save_lonb.shape)\n",
    "print(time_in_nsb.shape)\n",
    "print(data_to_save_idb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f71cb6-8f77-4ad6-a7e3-549017c5fdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create file /global/homes/k/kefalc/code/data_files/Final/floats_years_2010_2019_temperature_salinity_oxygen_10daily_SO_30S.nc\n",
      "min/max lon saved: -179.99962601111304 179.99986215794988\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#-----\n",
    "# save as netcdf file: field at several depth levels (for testing of Kristen's routines)\n",
    "#-----\n",
    "# I a mimicking the data format of the example file Kristen used\n",
    "\n",
    "fv = np.nan #-999\n",
    "    \n",
    "save_netcdf_multiple_depths = True\n",
    "if save_netcdf_multiple_depths:\n",
    "    if reduce_floats:\n",
    "        reduced_where = '_keep_random_'+str(keep_how_many)+'perc'\n",
    "        netcdf_name = 'floats_years_'+str(year_list[0])+'_'+str(year_list[-1])+\\\n",
    "                    '_'+vari1_nc+'_'+vari2_nc+'_'+vari3_nc+'_'+str(xx_daily)+'daily_'+region_string+reduced_where+'.nc'\n",
    "    else:\n",
    "        netcdf_name = 'floats_years_'+str(year_list[0])+'_'+str(year_list[-1])+\\\n",
    "                    '_'+vari1_nc+'_'+vari2_nc+'_'+vari3_nc+'_'+str(xx_daily)+'daily_'+region_string+'.nc'\n",
    "    \n",
    "    if not os.path.exists(savepath+netcdf_name):\n",
    "        print('Create file '+savepath+netcdf_name)\n",
    "        w_nc_fid = Dataset(savepath+netcdf_name, 'w', format='NETCDF4_CLASSIC')\n",
    "        w_nc_fid.contact = 'Kristen Falcinelli, kefalc@uw.edu'\n",
    "        w_nc_fid.source_file = path1\n",
    "        w_nc_fid.script    = '/global/homes/k/kefalc/code/gpr-mapping-data/save_floats_E3SM_mapping_idealized_distributions.ipynb'\n",
    "        # create dimension & variable\n",
    "        w_nc_fid.createDimension('profile', data_to_save1b.shape[0]) \n",
    "        w_nc_fid.createDimension('pressure', data_to_save1b.shape[1]) \n",
    "        w_nc_var1 = w_nc_fid.createVariable(vari1_nc, 'f8',('pressure','profile'),fill_value=fv)\n",
    "        w_nc_var1.description = str(xx_daily)+'-daily '+vari_temp+' on synthetic E3SM floats for the years '+\\\n",
    "        str(year_list[0])+'-'+str(year_list[-1])\n",
    "        w_nc_var1.units = unit1\n",
    "        w_nc_var1.coordinates = \"lat lon time floatid\"\n",
    "        w_nc_var1 = w_nc_fid.createVariable(vari2_nc, 'f8',('pressure','profile'),fill_value=fv)\n",
    "        w_nc_var1.description = str(xx_daily)+'-daily '+vari_salt+' on synthetic E3SM floats for the years '+\\\n",
    "                    str(year_list[0])+'-'+str(year_list[-1])\n",
    "        w_nc_var1.units = unit2\n",
    "        w_nc_var1.coordinates = \"lat lon time floatid\"\n",
    "        \n",
    "##Oxygen or other bgc variable- comment out for TS steps.\n",
    "        w_nc_var1 = w_nc_fid.createVariable(vari3_nc, 'f8',('pressure','profile'),fill_value=fv)\n",
    "        w_nc_var1.description = str(xx_daily)+'-daily '+vari_oxy+' on synthetic E3SM floats for the years '+\\\n",
    "                       str(year_list[0])+'-'+str(year_list[-1])\n",
    "        w_nc_var1.units = unit3\n",
    "        w_nc_var1.coordinates = \"lat lon time floatid\"\n",
    "    \n",
    "        w_nc_var1 = w_nc_fid.createVariable('lat', 'f8',('profile'),fill_value=fv)\n",
    "        w_nc_var1.description = 'Latitude'\n",
    "        w_nc_var1.units = 'deg N'\n",
    "        w_nc_var1 = w_nc_fid.createVariable('lon', 'f8',('profile'),fill_value=fv)\n",
    "        w_nc_var1.description = 'Longitude (-180:180)'\n",
    "        w_nc_var1.units = 'deg E'\n",
    "        w_nc_var1 = w_nc_fid.createVariable('pressure', 'f8',('pressure'),fill_value=fv)\n",
    "        w_nc_var1.description = 'depth'\n",
    "        w_nc_var1.units = 'm'\n",
    "        w_nc_var1 = w_nc_fid.createVariable('time', 'f8',('profile'),fill_value=fv)\n",
    "        w_nc_var1.description = 'Time'\n",
    "        w_nc_var1.units = 'ns' #'ns since 1970-01-01 12:00'\n",
    "        w_nc_var1 = w_nc_fid.createVariable('floatid', 'f8',('profile'),fill_value=fv)\n",
    "        w_nc_var1.description = 'Float ID'\n",
    "        \n",
    "        w_nc_fid.close()\n",
    "                 \n",
    "    data_to_save1[np.isnan(data_to_save1)] = fv\n",
    "    data_to_save2[np.isnan(data_to_save2)] = fv\n",
    "    data_to_save3[np.isnan(data_to_save3)] = fv\n",
    "    data_to_save_lat[np.isnan(data_to_save_lat)] = fv\n",
    "    data_to_save_lon[np.isnan(data_to_save_lon)] = fv\n",
    "    time_in_ns[np.isnan(time_in_ns)] = fv\n",
    "   # data_to_save_id[np.isnan(data_to_save_id)] = fv\n",
    "\n",
    "    # convert lon to -180 to 180\n",
    "    data_to_save_lonb[data_to_save_lonb>180] = data_to_save_lonb[data_to_save_lonb>180]-360\n",
    "    print('min/max lon saved:',np.min(data_to_save_lonb),np.max(data_to_save_lonb))\n",
    "    \n",
    "    w_nc_fid = Dataset(savepath+netcdf_name, 'r+', format='NETCDF4_CLASSIC') \n",
    "    w_nc_fid.variables[vari1_nc][:,:] = np.transpose(data_to_save1b)\n",
    "    w_nc_fid.variables[vari2_nc][:,:] = np.transpose(data_to_save2b)\n",
    "    w_nc_fid.variables[vari3_nc][:,:] = np.transpose(data_to_save3b)    \n",
    "    w_nc_fid.variables['lat'][:]  = data_to_save_latb\n",
    "    w_nc_fid.variables['lon'][:]  = data_to_save_lonb\n",
    "    w_nc_fid.variables['time'][:] = time_in_nsb\n",
    "    w_nc_fid.variables['pressure'][:] = depth_interp\n",
    "    w_nc_fid.variables['floatid'][:] = data_to_save_id\n",
    "    w_nc_fid.close()  \n",
    "                \n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05291da0-c552-46a6-8e99-99242fdf0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nans from the existing data set and save without NaNs\n",
    "ds = xr.open_dataset('/global/homes/k/kefalc/code/data_files/Final/floats_years_2010_2019_temperature_salinity_oxygen_10daily_SO_30S.nc')\n",
    "ds = ds.dropna(dim=\"profile\", how = \"any\")\n",
    "ds.to_netcdf('/global/homes/k/kefalc/code/data_files/Final/floats_years_2010_2019_temperature_salinity_oxygen_10daily_SO_30S_edit.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae1197e-e4df-4174-9edf-94de5fc521d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "70147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "70147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIONAL: further reduce the data coverage\n",
    "# REDUCE FLOATS BY FLOAT NUMBER\n",
    "# This is better for real world situations. \n",
    "\n",
    "ds = xr.open_dataset('/global/homes/k/kefalc/code/data_files/Final/floats_years_2010_2019_temperature_salinity_oxygen_10daily_SO_30S_edit.nc')\n",
    "floats = np.unique(ds.floatid.values)\n",
    "floats = sorted(floats)\n",
    "n = len(floats)\n",
    "\n",
    "keep_how_many = 15 #in percent\n",
    "keepfloats = round(n*(keep_how_many/100))\n",
    "display(keepfloats)\n",
    "\n",
    "reduced = random.sample(floats, keepfloats)\n",
    "\n",
    "floatid = ds[\"floatid\"].to_index()\n",
    "\n",
    "#select indexes\n",
    "index = []\n",
    "for i in range(len(reduced)):\n",
    "    index = np.append(index, np.where((reduced[i] == floatid)\n",
    "                                     ))\n",
    "# display(index_bgc)\n",
    "#index=np.unique(index)\n",
    "display(len(index)) #2877\n",
    "display(len(np.unique(index)))\n",
    "#display(index[0:100])\n",
    "\n",
    "#Now I need to get rid of the indexes I do not need in the file. \n",
    "index = index.astype(int) ##These are the indexes we are keepign in the file!\n",
    "\n",
    "ds_new = ds.isel(profile = index)\n",
    "ds_new.to_netcdf(savepath + 'floats_years_'+str(year_list[0])+'_'+str(year_list[-1])+\\\n",
    "                    '_'+vari1_nc+'_'+vari2_nc+'_'+vari3_nc+'_'+str(xx_daily)+'daily_'+region_string+'keep_random_floats_'+str(keep_how_many)+'perc.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3104a-3898-409f-838b-2c9e1d7ef484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---\n",
    "# plot a map with all available observations\n",
    "#---\n",
    "\n",
    "ms = 0.7\n",
    "color1 = 'darkblue'\n",
    "color2 = 'limegreen'\n",
    "fs = 10\n",
    "\n",
    "save_plots = True\n",
    "\n",
    "fig = plt.figure(figsize=(18,7)) # x, y\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.stock_img()\n",
    "ax.coastlines()\n",
    "ax.set_extent([-150, -120, -90, latmax+5])\n",
    "for nn in tqdm(range(0,data_to_save_lonb.shape[0])):\n",
    "    plt.plot(data_to_save_lonb[nn], data_to_save_latb[nn],\n",
    "                 color=color1, marker='o',markersize=ms,transform=ccrs.Geodetic())\n",
    "ax.yaxis.tick_left()\n",
    "#gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "#                  linewidth=0.1, color='gray', alpha=0.5, linestyle='None')\n",
    "#gl.xlabel_style = {'size': 12, 'color': 'k'}\n",
    "#gl.ylabel_style = {'color': 'red', 'weight': 'bold', 'size':12}\n",
    "#gl.top_labels = False\n",
    "#gl.left_labels = False\n",
    "ax.set_xticks([-150,-140,-130, -120],crs=ccrs.PlateCarree())\n",
    "ax.set_yticks([-60, -30], crs=ccrs.PlateCarree())\n",
    "lon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "lat_formatter = LatitudeFormatter()\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "#ax.xaxis.label.set_color('red')\n",
    "#ax.yaxis.label.set_color('red')\n",
    "ax.annotate(str(year1)+'-'+str(year2),xy=(0.2,0.15),\\\n",
    "                xycoords='axes fraction',fontsize=fs,ha='right',color='k',zorder=120,fontweight='bold')\n",
    "ax.annotate(str(xx_daily)+'-daily',xy=(0.2,0.07),\\\n",
    "                xycoords='axes fraction',fontsize=fs,ha='right',color='k',zorder=120)#,fontweight='bold')\n",
    "ax.annotate(str(data_to_save_lonb.shape[0])+' data points',xy=(0.92,0.15),\\\n",
    "                xycoords='axes fraction',fontsize=fs,ha='right',color='k',zorder=120,fontweight='bold')\n",
    "ax.annotate('(of '+str(len(data_to_save_lon))+' in total)',xy=(0.92,0.07),\\\n",
    "                xycoords='axes fraction',fontsize=fs,ha='right',color='k',zorder=120)#,fontweight='bold')\n",
    "if save_plots:\n",
    "    dpicnt = 200\n",
    "    if reduce_floats:\n",
    "        reduced_where = '_keep_random_'+str(keep_how_many)+'perc'\n",
    "        filename = 'Map_floats_years_'+str(year_list[0])+'_'+str(year_list[-1])+\\\n",
    "                    '_'+vari1_nc+'_'+vari2_nc+'_'+vari3_nc+'_'+str(xx_daily)+'daily_'+region_string+reduced_where+'.png'\n",
    "    else:\n",
    "        filename = 'Map_floats_years_'+str(year_list[0])+'_'+str(year_list[-1])+\\\n",
    "                    '_'+vari1_nc+'_'+vari2_nc+'_'+vari3_nc+'_'+str(xx_daily)+'daily_'+region_string+'.png'\n",
    "    print(filename)\n",
    "    #plt.savefig(savepath_plots+filename,dpi = dpicnt, bbox_inches='tight',format='png')#,transparent=True)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9f48d-f9c6-4cf9-855d-fbd2f4ecf480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/global/homes/k/kefalc/code/data_files/floats_years_2011_2020_temperature_salinity_oxygen_10daily_SO_Test_edit.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92ab96-99d1-4c6f-8122-dbc3c9a3e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(ds.floatid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee2730-55b7-4f5d-b456-f92083c3a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tplot = plt.scatter(ds.lon, ds.lat, c = ds.time)\n",
    "cbar = plt.colorbar(tplot)\n",
    "#plt.savefig('/global/homes/k/kefalc/code/plots/timeplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52adc4-b7a6-4251-951c-56d19cb79967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
